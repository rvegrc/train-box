services:
  ollama:
    container_name: ollama
    volumes:
      - ./backend/data/ollama:/root/.ollama
      - ../models/:/custom/models
    environment:
      - OLLAMA_MODELS=/custom/models
      - OLLAMA_SERVER_HOST=0.0.0.0
    ports:
      - "21434:11434"
    pull_policy: always
    tty: true
    restart: unless-stopped
    image: ollama/ollama:${OLLAMA_DOCKER_TAG-latest}
    # image: ollama/ollama:0.5.8
    networks:
      - llmnet 

  open-webui:
    image: ghcr.io/open-webui/open-webui:${WEBUI_DOCKER_TAG-main}
    container_name: open-webui
    volumes:
      - ./frontend/data/open-webui:/app/backend/data
    depends_on:
      - ollama
    ports:
      - ${OPEN_WEBUI_PORT-3000}:8080
    env_file:
      - .env
      # - 'OLLAMA_BASE_URL='
      # - 'WEBUI_SECRET_KEY='
    # extra_hosts:
    #   - host.docker.internal:host-gateway
    restart: unless-stopped
    networks:
      - llmnet 

      
  qdrant:
    # image: qdrant/qdrant
    image: qdrant/qdrant:v1.15
    container_name: qdrant
    volumes:
      - ./qdrant_data:/qdrant/storage:z
    ports:
      - "26333:6333"
      - "26334:6334"
    restart: unless-stopped
    networks:
      - llmnet 



# # If you want to serve multiple models under /models/, you need to start multiple vLLM instances, each with a different --model argument 

  # vllm_Qwen3-0.6B:
  #   image: vllm/vllm-openai:latest
  #   container_name: vllm_Qwen3-0.6B
  #   restart: unless-stopped
  #   ports:
  #     - "28001:8000"  # OpenAI-compatible API endpoint
  #   volumes:
  #     - ../models:/models  # put your Hugging Face or converted model here
  #   command: >
  #     --model /models/Qwen3-0.6B
  #     --host 0.0.0.0
  #     --port 8000    
  #   # Optional: enable GPU
  #   # deploy:
  #   #   resources:
  #   #     reservations:
  #   #       devices:
  #   #         - capabilities: [gpu]
  #   networks:
  #     - llmnet

  vllm_multilingual-e5-large-instruct:
    image: vllm/vllm-openai:latest
    container_name: vllm_multilingual-e5-large-instruct
    restart: unless-stopped
    ports:
      - "28000:8000"  # OpenAI-compatible API endpoint
    volumes:
      - ../models:/models  # put your Hugging Face or converted model here
    command: >
      --model /models/multilingual-e5-large-instruct
      --host 0.0.0.0
      --port 8000
      --device cpu
    environment:
      - VLLM_LOGGING_LEVEL=DEBUG  # Set logging level, default is INFO
    # Optional: enable GPU
    # deploy:
    #   resources:
    #     reservations:
    #       devices:
    #         - capabilities: [gpu]
    networks:
      - llmnet

#   open-webui:
#     image: ghcr.io/open-webui/open-webui:main
#     container_name: open-webui
#     restart: unless-stopped
#     depends_on:
#       - vllm
#     ports:
#       - "3000:8080"
#     environment:
#       - OPENAI_API_BASE=http://vllm:8000/v1
#       - OPENAI_API_KEY=sk-no-key-required  # vLLM doesn't require auth by default
#       # - WEBUI_SECRET_KEY=replace_with_a_strong_secret
#     volumes:
#       - ./openwebui_data:/app/backend/data
#     networks:
#       - llmnet    


networks:
  llmnet:
    driver: bridge

volumes:
  ollama: {}
  open-webui: {}
  qdrant_data: {}