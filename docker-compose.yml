services:
  ollama:
    container_name: ollama
    volumes:
      - ./backend/data/ollama:/root/.ollama
      - ../models/:/custom/models
    environment:
      - OLLAMA_MODELS=/custom/models
      - OLLAMA_SERVER_HOST=0.0.0.0
    ports:
      - "21434:11434"
    pull_policy: always
    tty: true
    restart: unless-stopped
    image: ollama/ollama:${OLLAMA_DOCKER_TAG-latest}
    # image: ollama/ollama:0.5.8
    networks:
      - llmnet 

  open-webui:
    image: ghcr.io/open-webui/open-webui:${WEBUI_DOCKER_TAG-main}
    container_name: open-webui
    volumes:
      - ./frontend/data/open-webui:/app/backend/data
    depends_on:
      - ollama
    ports:
      - ${OPEN_WEBUI_PORT-3000}:8080
    env_file:
      - .env
      # - 'OLLAMA_BASE_URL='
      # - 'WEBUI_SECRET_KEY='
    # extra_hosts:
    #   - host.docker.internal:host-gateway
    restart: unless-stopped
    networks:
      - llmnet 

      
  qdrant:
    # image: qdrant/qdrant
    image: qdrant/qdrant:v1.15
    container_name: qdrant
    volumes:
      - ./qdrant_data:/qdrant/storage:z
    ports:
      - "26333:6333"
      - "26334:6334"
    restart: unless-stopped
    networks:
      - llmnet 

networks:
  llmnet:
    driver: bridge

volumes:
  ollama: {}
  open-webui: {}
  qdrant_data: {}


# # If you want to serve multiple models under /models/, you need to start multiple vLLM instances, each with a different --model argument 

#   vllm:
#     image: vllm/vllm-openai:latest
#     container_name: vllm
#     restart: unless-stopped
#     ports:
#       - "28000:8000"  # OpenAI-compatible API endpoint
#     volumes:
#       - ../models:/models  # put your Hugging Face or converted model here
#     command: >
#       --model /models/Qwen3-0.6B
#       --host 0.0.0.0
#       --port 8000    
#     # Optional: enable GPU
#     # deploy:
#     #   resources:
#     #     reservations:
#     #       devices:
#     #         - capabilities: [gpu]
#     networks:
#       - llmnet

#   open-webui:
#     image: ghcr.io/open-webui/open-webui:main
#     container_name: open-webui
#     restart: unless-stopped
#     depends_on:
#       - vllm
#     ports:
#       - "3000:8080"
#     environment:
#       - OPENAI_API_BASE=http://vllm:8000/v1
#       - OPENAI_API_KEY=sk-no-key-required  # vLLM doesn't require auth by default
#       # - WEBUI_SECRET_KEY=replace_with_a_strong_secret
#     volumes:
#       - ./openwebui_data:/app/backend/data
#     networks:
#       - llmnet    
